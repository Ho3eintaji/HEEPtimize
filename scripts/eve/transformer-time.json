{
  "norm": {
    "120x400": { 
      "cpu": 0,
      "_comment": "2: norm before initial embedding",
      "_num_rpt": 1
    },
    "120x16": {
      "cpu": 0,
      "carus": 0,
      "_comment": "5: norm after initial embedding",
      "_num_rpt": 1
    },
    "_120x16": {
      "cpu": 0,
      "carus": 0,
      "_comment": "8: norm after posEmbedding",
      "_num_rpt": 4
    },
    "121x16": {
      "cpu": 0,
      "carus": 0,
      "_comment": "20: norm after first FC and adding input",
      "_num_rpt": 4
    },
    "_121x16": {
      "cpu": 0,
      "carus": 0,
      "_comment": "27: norm after last FC and adding input",
      "_num_rpt": 4
    }
  },
  "add": {
    "120x16": {
      "cpu": 0,
      "carus": 0,
      "_comment": "4: add bias after initial embedding matmul",
      "_num_rpt": 1
    },
    "121x4": {
      "cpu": 0,
      "carus": 0,
      "_comment": "10: addbias for first 3 matmul in self-attention begining",
      "_num_rpt": 48
    },
    "121x16": {
      "cpu": 0,
      "carus": 0,
      "_comment": "18: addbias for first FC",
      "_num_rpt": 4
    },
    "_121x16": {
      "cpu": 0,
      "carus": 0,
      "_comment": "19: add input after first FC",
      "_num_rpt": 4
    },
    "_121x4": {
      "cpu": 0,
      "carus": 0,
      "_comment": "22: addbias for second FC",
      "_num_rpt": 4
    },
    "__121x16": {
      "cpu": 0,
      "carus": 0,
      "_comment": "25: addbias for last FC",
      "_num_rpt": 4
    },
    "___121x16": {
      "cpu": 0,
      "carus": 0,
      "_comment": "26: add input after last FC",
      "_num_rpt": 4
    },
    "1x16": {
      "cpu": 0,
      "carus": 0,
      "_comment": "29: add bias after classification",
      "_num_rpt": 1
    }
  },
  "clsConcatenate": {
    "120x16": {
      "cpu": 0,
      "_comment": "6: clsConcatenate",
      "_num_rpt": 1
    }
  },
  "posEmbedding": {
    "120x16": {
      "cpu": 0,
      "_comment": "7: posEmbedding",
      "_num_rpt": 1
    }
  },
  "matmul_bias": {
  },
  "gelu": {
    "121x4": {
      "cpu": 2287166,
      "_comment": "23: gelu after FC",
      "_num_rpt": 4
    }
  },
  "matmul": {
    "120x400x16": {
      "cpu": 0,
      "carus": 0,
      "_comment": "3: initial embedding",
      "_num_rpt": 1
    },
    "121x16x4": {
      "cpu": 0,
      "carus": 0,
      "_comment": "9: first 3 matmul in self-attention begining",
      "_num_rpt": 48
    },
    "121x4x121": {
      "cpu": 0,
      "carus": 0,
      "_comment": "13: QxK^T",
      "_num_rpt": 16
    },
    "121x121x4": {
      "cpu": 0,
      "carus": 0,
      "_comment": "15: softMax(Q x K^T) x V",
      "_num_rpt": 16
    },
    "121x16x16": {
      "cpu": 0,
      "carus": 0,
      "_comment": "17: first FC",
      "_num_rpt": 4
    },
    "_121x16x4": {
      "cpu": 0,
      "carus": 0,
      "_comment": "21: second FC",
      "_num_rpt": 4
    },
    "121x4x16": {
      "cpu": 0,
      "carus": 0,
      "_comment": "24: last FC",
      "_num_rpt": 4
    },
    "1x16x61": {
      "cpu": 0,
      "carus": 0,
      "_comment": "28: classification on token seq",
      "_num_rpt": 1
    }
  },
  "mm_scale": {
    "121x4": {
      "cpu": 4382,
      "_comment": "12: scale (divide by 2) of K^T",
      "_num_rpt": 16
    }
  },
  "mh_transpose": {
    "121x4x4": {
      "cpu": 18299,
      "_comment": "16: transposing after all the heads in self-attention",
      "_num_rpt": 4
    }
  },
  "transpose": {
    "121x4":{ 
      "cpu": 3925,
      "_comment": "11: K^T",
      "_num_rpt": 16
    }

  },
  "softmax": {
    "121x121": {
      "cpu": 40481549,
      "_comment": "14: softmax after Q x K^T",
      "_num_rpt": 16
    }
  }
}